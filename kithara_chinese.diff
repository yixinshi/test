diff --git a/examples/example_datasets.py b/examples/example_datasets.py
index fab2c4a..c18d4e8 100644
--- a/examples/example_datasets.py
+++ b/examples/example_datasets.py
@@ -34,9 +34,11 @@ def example_datasets(option: str) -> Tuple[ray.data.Dataset, ray.data.Dataset]:
 
 def _load_huggingface_dataset() -> Tuple[Dataset, Dataset]:
     """Load the C4 dataset from HuggingFace."""
-    hf_train_dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)
+    #hf_train_dataset = load_dataset("yixinshi/chinese20k", "en", split="train", streaming=True)
+    hf_train_dataset = load_dataset("yixinshi/chinese20k", split="train", streaming=True)
     hf_val_dataset = load_dataset(
-        "allenai/c4", "en", split="validation", streaming=True
+        "yixinshi/chinese20k", split="train", streaming=True
+        #"yixinshi/chinese20k", "en", split="validation", streaming=True
     )
 
     return (
diff --git a/examples/multihost/ray/TPU/sft_lora_example.py b/examples/multihost/ray/TPU/sft_lora_example.py
index 1a3f0ef..9416968 100644
--- a/examples/multihost/ray/TPU/sft_lora_example.py
+++ b/examples/multihost/ray/TPU/sft_lora_example.py
@@ -43,12 +43,12 @@ def main(train_ds, eval_ds, split_data_across_host):
     from examples.singlehost.sft_lora_example import run_workload
     run_workload(
         train_ds,
-        eval_ds,
+        eval_source=None,
         dataset_is_sharded_per_host=split_data_across_host,
     )
 
 # Create mulit-host datasets
-train_ds, eval_ds = example_datasets(option = "sft_toy")
+train_ds, eval_ds = example_datasets(option = "hf")
 split_data_across_host =  False
 if split_data_across_host: 
     train_ds: List[Any] = split_dataset(train_ds, num_hosts=num_tpu_hosts)
diff --git a/examples/singlehost/sft_lora_example.py b/examples/singlehost/sft_lora_example.py
index a8cf65c..c9d3122 100644
--- a/examples/singlehost/sft_lora_example.py
+++ b/examples/singlehost/sft_lora_example.py
@@ -37,7 +37,7 @@ config = {
     "lora_rank": 4,
     "precision": "mixed_bfloat16",
     "training_steps": 100,
-    "eval_steps_interval": 10,
+    "eval_steps_interval": 0,
     "log_steps_interval": 10,
     "per_device_batch_size": 1,
     "max_eval_samples": 50,
@@ -70,10 +70,14 @@ def run_workload(
         train_source,
         tokenizer=tokenizer,
         max_seq_len=config["seq_len"],
+        column_mapping={"prompt":"input", "answer": "output"},
     )
-    eval_dataset = SFTDataset(
-        eval_source, tokenizer=tokenizer, max_seq_len=config["seq_len"]
-    )
+    if eval_source:
+      eval_dataset = SFTDataset(
+          eval_source, tokenizer=tokenizer, max_seq_len=config["seq_len"]
+      )
+    else:
+      eval_dataset = None
 
     # Create optimizer
     optimizer = keras.optimizers.AdamW(learning_rate=5e-5, weight_decay=0.01)
@@ -84,11 +88,15 @@ def run_workload(
         per_device_batch_size=config["per_device_batch_size"],
         dataset_is_sharded_per_host=dataset_is_sharded_per_host,
     )
-    eval_dataloader = Dataloader(
-        eval_dataset,
-        per_device_batch_size=config["per_device_batch_size"],
-        dataset_is_sharded_per_host=dataset_is_sharded_per_host,
-    )
+
+    if eval_source:
+        eval_dataloader = Dataloader(
+            eval_dataset,
+            per_device_batch_size=config["per_device_batch_size"],
+            dataset_is_sharded_per_host=dataset_is_sharded_per_host,
+        )
+    else:
+        eval_dataloader = None
 
     # Initialize trainer
     trainer = Trainer(
@@ -104,12 +112,35 @@ def run_workload(
 
     # Start training
     trainer.train()
+    print("Yixin: training done 2025!")
 
     # Test after tuning
-    pred = model.generate(
-        "What is your name?", max_length=30, tokenizer=tokenizer, return_decoded=True
-    )
-    print("Tuned model generates:", pred)
+    print("1"*200)
+    alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
+                   ### Instruction:
+                   {}
+
+                   ### Input:
+                   {}
+
+                   ### Response: {}"""
+    english_only_prompt = alpaca_prompt.format("Please translate!", "English only", "")
+    alpaca_prompt = alpaca_prompt.format("Please translate!", "主公要吃饭去", "")
+    pred = model.generate("what is your name?", max_length=30, tokenizer=tokenizer, return_decoded=True)
+    print("yxshi:  Simple output:", pred)
+
+    print("2"*200)
+    print("Yixin:English only input: {}".format(english_only_prompt))
+    pred = model.generate(english_only_prompt, max_length=400, tokenizer=tokenizer, return_decoded=True)
+    print("yxshi:  English only output:", pred)
+
+    print("3"*200)
+    print("Yixin: Input for large predicton: {}".format(alpaca_prompt))
+    pred = model.generate(alpaca_prompt, max_length=400, tokenizer=tokenizer, return_decoded=True)
+    print("yxshi: Tuned model generates:", pred)
+
+    print("4"*200)
+    print ("Yixin: Prediction done 2026")
 
 
 if __name__ == "__main__":
diff --git a/kithara/dataset/sft.py b/kithara/dataset/sft.py
index c8b00ee..090bd11 100644
--- a/kithara/dataset/sft.py
+++ b/kithara/dataset/sft.py
@@ -72,11 +72,21 @@ class SFTDataset(TextCompletionDataset):
 
         """
         prompt, answer = sample["prompt"], sample["answer"]
+        alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
+                           ### Instruction:
+                           {}
+
+                           ### Input:
+                           {}
+
+                           ### Response: {}"""
+        alpaca_prompt_full = alpaca_prompt.format("Please translate!", prompt, answer)
         full_seq = HFtokenize(
-            f"<bos>{prompt}{answer}<eos>", self.tokenizer, seq_len=self.max_seq_len
+            f"<bos>{alpaca_prompt_full}<eos>", self.tokenizer, seq_len=self.max_seq_len
         )
+        alpaca_prompt_prompt = alpaca_prompt_full[:-len(answer)]
         prompt_seq = HFtokenize(
-            f"<bos>{prompt}",
+            f"<bos>{alpaca_prompt_prompt}",
             self.tokenizer,
             seq_len=self.max_seq_len,
             padding="do_not_pad",
diff --git a/kithara/trainer/trainer.py b/kithara/trainer/trainer.py
index 55c79db..ca1abef 100644
--- a/kithara/trainer/trainer.py
+++ b/kithara/trainer/trainer.py
@@ -243,13 +243,14 @@ class Trainer:
                 self.callbacks.on_train_batch_end(self.step_count, {"loss": loss})
 
                 # Periodic evaluation
-                if self.step_count % self.eval_steps_interval == 0:
+                if self.eval_dataloader and self.step_count % self.eval_steps_interval == 0:
                     self.evaluate(state)
             # Compute epoch statistics
             epoch_loss = epoch_loss / train_set_size
             self.callbacks.on_epoch_end(self.epoch_count, {"epoch_loss": epoch_loss})
             print(f"Train epoch {self.epoch_count} loss : {epoch_loss}")
 
+        print("yxshi: training done! calling callbacks on_train_end")
         self.callbacks.on_train_end()
 
     def save_model(self, filepath):
@@ -518,7 +519,3 @@ class Trainer:
         assert (
             self.max_eval_samples >= self.global_batch_size
         ), "Number of eval examples must be greater or equal to global batch size"
-
-        assert not (
-            self.eval_steps_interval != sys.maxsize and self.eval_dataloader is None
-        ), "Evaluation steps interval is set but no eval dataloader is provided"
